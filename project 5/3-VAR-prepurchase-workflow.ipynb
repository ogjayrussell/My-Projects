{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dac751f",
   "metadata": {},
   "source": [
    "# Goal: To create a time series model that will predict future prepurchase 12 months in advance, at the state level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd9dac",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0b3824-ad76-40b7-a34c-be733d6771a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fb4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/state_level_metrics.csv', parse_dates = ['index'], index_col = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c10e5d-6794-47f3-b2ae-de233a0ec3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'district_of_columbia', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan', 'minnesota', \n",
    "              'mississippi', 'missouri', 'montana', 'nebraska', 'nevada', 'new_hampshire', 'new_jersey', 'new_mexico', 'new_york', 'north_carolina', 'north_dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode_island', 'south_carolina', 'south_dakota', 'tennessee', 'texas', 'utah', \n",
    "              'vermont', 'virginia', 'washington', 'west_virginia', 'wisconsin', 'wyoming']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc36f3",
   "metadata": {},
   "source": [
    "# Minor cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434e6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ebd226-7196-4460-938c-da00ab785112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing the index interval\n",
    "def set_intervals(df, interval = str):\n",
    "    df = df.resample(interval).ffill().interpolate('linear').dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9528250e-4bc4-4c4f-9398-d38283f8c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = set_intervals(df, 'M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea391b63",
   "metadata": {},
   "source": [
    "#### Only using certain features to predict prepurchase. If we used the same features to predict delinquencies and prepurchase, the model would be the same and interpretability becomes even more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378a1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features that we don't want to use for delinquency prediction\n",
    "df = df[df.columns.drop(list(df.filter(regex='shelter|population|rate')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3dbb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_indices(df):\n",
    "    \"\"\"Returns a dictionary of dataframes separating features by state. Each value in the dictionary is a state-represented dataframe that will be iterated over later.\"\"\"\n",
    "    df_dict = {}\n",
    "    df_list = []\n",
    "\n",
    "    states = ['alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', 'district_of_columbia', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan', 'minnesota', \n",
    "              'mississippi', 'missouri', 'montana', 'nebraska', 'nevada', 'new_hampshire', 'new_jersey', 'new_mexico', 'new_york', 'north_carolina', 'north_dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode_island', 'south_carolina', 'south_dakota', 'tennessee', 'texas', 'utah', \n",
    "              'vermont', 'virginia', 'washington', 'west_virginia', 'wisconsin', 'wyoming']\n",
    "    \n",
    "    for state in states:\n",
    "        df_list.append(df.filter(regex=f'{state}', axis = 1))\n",
    "    \n",
    "    for i in range(len(states)):\n",
    "        df_dict[states[i]] = df_list[i]\n",
    "\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea20eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = state_indices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e780391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing west virginia and arkansas from virginia and kansas dataframes\n",
    "df_dict['virginia'] = df_dict['virginia'][df_dict['virginia'].columns.drop(list(df_dict['virginia'].filter(regex='west_virginia')))]\n",
    "df_dict['kansas'] = df_dict['kansas'][df_dict['kansas'].columns.drop(list(df_dict['kansas'].filter(regex='arkansas')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb5a20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, df in df_dict.items():\n",
    "    df_dict[state] = utils.create_indices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5abd4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code written by Joseph Nelson\n",
    "def interpret_dftest(dftest):\n",
    "    dfoutput = pd.Series(dftest[0:2], index=['Test Statistic', 'p-value'])\n",
    "    return dfoutput\n",
    "\n",
    "# achieve stationarity for all columns, can only do it iteratively for 1 diff without going into recursion or something more complex\n",
    "# also the var model won't run if we do full stationarity, columns result in having constants\n",
    "def stationarity(df):\n",
    "    for col in df:\n",
    "        df[col] = df[col].diff()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16742e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, df in df_dict.items():\n",
    "    df_dict[state] = stationarity(df).fillna(0)\n",
    "    # Removes any columns with constant zeros\n",
    "    df_dict[state] = df_dict[state].loc[:, (df_dict[state] != df_dict[state].iloc[0]).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cf12bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_model(dictionary):    \n",
    "    forecast_df = pd.DataFrame(columns=[\n",
    "        'state', \n",
    "        'current', \n",
    "        'forecast', \n",
    "        'rmse_baseline', \n",
    "        'rmse_train', \n",
    "        'rmse_test', \n",
    "        'mae_baseline', \n",
    "        'mae_train', \n",
    "        'mae_test',\n",
    "        'mape_baseline',\n",
    "        'mape_train',\n",
    "        'mape_test'])\n",
    "\n",
    "    for state, df in dictionary.items():\n",
    "\n",
    "        train, test = train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        shuffle=False)\n",
    "\n",
    "        model = VAR(train)\n",
    "        ts_model = model.fit(maxlags=1, ic='aic')  \n",
    "\n",
    "        # VAR Model Eval Metrics\n",
    "        train_rmse = np.sqrt(mean_squared_error(train.values, ts_model.forecast(train.values, len(train))))\n",
    "        test_rmse = np.sqrt(mean_squared_error(test.values, ts_model.forecast(test.values, len(test))))\n",
    "        train_mae = mean_absolute_error(train.values, ts_model.forecast(train.values, len(train)))\n",
    "        test_mae = mean_absolute_error(test.values, ts_model.forecast(test.values, len(test)))\n",
    "        train_mape = mean_absolute_percentage_error(train.values, ts_model.forecast(train.values, len(train)))\n",
    "        test_mape = mean_absolute_percentage_error(test.values, ts_model.forecast(test.values, len(test)))\n",
    "\n",
    "        # Baseline (12 month rolling average) Eval Metrics\n",
    "        # Using a 12 month rolling average as a baseline as we can't use mean the same way as we would if we were doing a standard regression task\n",
    "        baseline_df = df[-29:].rolling(12).sum().fillna(0)\n",
    "        baseline_rmse = np.sqrt(mean_squared_error(test.values, baseline_df))\n",
    "        baseline_mae = mean_absolute_error(test.values, baseline_df)\n",
    "        baseline_mape = mean_absolute_percentage_error(test.values, baseline_df)\n",
    "        \n",
    "        # Add values to df\n",
    "        forecast_df.loc[state] = [\n",
    "            state, # state\n",
    "            df['made_index'][-1], # current\n",
    "            ts_model.forecast(test.values, 12)[11][3], # forecast\n",
    "            round(baseline_rmse, 2), # rmse_baseline\n",
    "            round(train_rmse, 2), # rmse_train\n",
    "            round(test_rmse, 2), # rmse_test\n",
    "            round(baseline_mae, 2), # mae_baseline\n",
    "            round(train_mae, 2), # mae_train\n",
    "            round(test_mae, 2), # mae_test \n",
    "            round(baseline_mape, 2), # mape_baseline\n",
    "            round(train_mape, 2), # mape_train\n",
    "            round(test_mape, 2), # mape_test \n",
    "        ] # rounding to prevent scientific notation, readability\n",
    "        # Forecasting 12 months ahead as the dataframe's time intervals are monthly\n",
    "\n",
    "    return forecast_df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a376d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = var_model(df_dict)\n",
    "# Output to csv for Tableau integration\n",
    "forecast_df.to_csv('./data/prepurchase_join_to_hud_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
