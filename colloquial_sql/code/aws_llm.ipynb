{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04e7206-3653-4358-b7e3-601fca8aa69b",
   "metadata": {},
   "source": [
    "# Training and Deploying Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e9a59",
   "metadata": {},
   "source": [
    "This notebook was copied from AWS SageMaker to give insight on the workflow on model configuration and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e298ae-9078-42a5-8122-50d219241aad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.0.0.post200)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f353a0d-076e-4360-8ba3-a697b954e749",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import io\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e22c6-9359-4bec-8b21-e75ad7e201d7",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "638c7f17-96e1-4131-b7ff-0fad2b67c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::566086704797:role/service-role/AmazonSageMaker-ExecutionRole-20240222T123141\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "#preparing the necessary AWS resources and permissions to ensure \n",
    "#SageMaker can access the data it needs and has the permissions to perform operations on behalf of the user.\n",
    "\n",
    "# sagemaker_session_bucket -> used for uploading data, models and logs\n",
    "# sagemaker_will_automatically create this bucket if it not exists\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e78887-d965-4789-b733-c367a4f3bb49",
   "metadata": {},
   "source": [
    "## Hugging Face Deep Learning Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3788ede-9d4d-473c-8f2e-489432416a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version=\"1.1.0\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34182998-bb69-48a6-adaf-7039d82a40af",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Mistral 7B instruct model is 15GB. With an instance of ml.g5.xlarge, I'm able to load the model since it is comprised of 1 GPU that has 24GB of VRAM (GPU memory). \n",
    "\n",
    "### Increasing computation efficiency - Quantisation (8bit)\n",
    "Reduces the precision of the model, and therefore reduces the memory footprint as well. There is a minimal loss in accuracy, but not enough to affect the model.\n",
    "Memory and Computation Reduction: Quantization reduces the precision of the model's weights and activations, which decreases the memory and computational requirements of the model. increases computational efficiency. increases inference speed which will be more suitable for user interaction with this app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39b0c82b-a99b-43b8-90fa-e4bc757c589f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "n_gpu = 1\n",
    "health_check_timeout = 800\n",
    "model_checkpoint = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "# Model configuration for text generation ingerence with huggingface\n",
    "config = {\n",
    "    'HF_MODEL_ID': f\"{model_checkpoint}\", # model_id for Mistral 7B. \n",
    "    'SM_NUM_GPUS': json.dumps(n_gpu),\n",
    "    'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "    'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "    'HUGGING_FACE_HUB_TOKEN': json.dumps(\"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\"),\n",
    "    'HF_MODEL_QUANTIZE': \"bitsandbytes\",  # Enable bitsandbytes quantization\n",
    "    'QUANTIZATION_BITS': json.dumps(8)  # Set quantization to 8-bit\n",
    "}\n",
    "\n",
    "# HF Model Class\n",
    "hf_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ce902-5347-4918-bdb9-25a4244c2ecc",
   "metadata": {},
   "source": [
    "## Deploy Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c091542c-94df-45d0-bdf6-d3c51c483cc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "# deploy the HuggingFaceModel to Amazon SageMaker\n",
    "# Creates an endpoint that will contain the model\n",
    "llm = hf_model.deploy(\n",
    "  initial_instance_count= 1,\n",
    "  instance_type= instance_type,\n",
    "  container_startup_health_check_timeout= health_check_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdaad3e-ebe2-43bd-b082-d391aaa2593b",
   "metadata": {},
   "source": [
    "## Structure and configure response\n",
    "The Mistral-7B-Instruct model, as opposed to its base counterpart, the Mistral7B, has undergone a process known as instruction fine-tuning. This specialized training enhances the model's ability to understand and follow instructions more effectively, making it significantly more adept for tasks requiring precise command adherence. The instruction fine-tuning process involved training the model on datasets that include specific sentence identifiers (IDs), which play a crucial role in guiding the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef23c79-2dfc-4e71-bac8-b55a6f25af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'what are the ingredients of a meat pie'\n",
    "prompt= f'''[INST] {message} [/INST]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c752230-fd25-4ead-9f8c-d946db7fb3e6",
   "metadata": {},
   "source": [
    "### Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e00e419-f764-4671-b927-a3a1980a943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config response\n",
    "input_data = {\n",
    "    'inputs': prompt,  # The prompt or input text that you want the model to generate a response for.\n",
    "    'parameters': {\n",
    "        'do_sample': True,  # Whether to use sampling; if False, the model will use greedy decoding.\n",
    "        'top_p': 0.6,  # The cumulative probability threshold for nucleus sampling. Only the smallest set of tokens with cumulative probability >= top_p are considered.\n",
    "        'temperature': 0.3,  # Controls the randomness of predictions by scaling the logits before applying softmax. Lower values make the model more deterministic.\n",
    "        'top_k': 50,  # The number of highest probability vocabulary tokens to keep for top-k filtering. Only the top_k tokens are considered for sampling.\n",
    "        'max_new_tokens': 512  # The maximum number of new tokens to generate in the response.\n",
    "        # 'repetition_penalty': 1.03  # Penalty for repeated tokens. Values > 1.0 penalize repetition, encouraging the model to produce more diverse outputs.\n",
    "    }\n",
    "}\n",
    "\n",
    "body_input_data_json = json.dumps(input_data)# needs to be in json format to be passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367aefc-b00d-4197-9298-f3da4705e79e",
   "metadata": {},
   "source": [
    "### Request the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de56640-7862-45bf-b720-3f2fcfc9be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker runtime client\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "endpoint = 'huggingface-pytorch-tgi-inference-2024-04-26-05-43-18-580'\n",
    "content_type = 'application/json'\n",
    "\n",
    "\n",
    "# Requests inference from AWS SageMaker endpoint\n",
    "# Model deployed - Mistral 7B\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName = endpoint,\n",
    "    ContentType = content_type,\n",
    "    Body = body_input_data_json.encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d70ade-6700-4b77-b1bb-c9d7b8571e9f",
   "metadata": {},
   "source": [
    "### Parse the response\n",
    "The response is given in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be25ab40-3d07-48d6-9cfd-88175332a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = response['Body'].read().decode('utf-8')\n",
    "response_json = json.loads(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05a96ee6-786c-4c03-994e-31bd9beb5eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '[INST] what are the ingredients of a meat pie [/INST] The ingredients for a meat pie can vary depending on the recipe, but some common ingredients include:\\n\\n* Flaky pie crust\\n* Ground beef, pork, or a combination of the two\\n* Onion\\n* Garlic\\n* Carrots\\n* Peas\\n* Thyme\\n* Rosemary\\n* Salt\\n* Pepper\\n* Egg wash (optional)\\n\\nSome recipes may also include other ingredients such as mushrooms, celery, or diced tomatoes.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470912c-77a4-4dde-9ccf-aac438fb7da2",
   "metadata": {},
   "source": [
    "### Extract and Print Response\n",
    "Lets see how the baseline Mistral7B model will perform without fine-tuning. This will be comparable to the fine-tuned model which has been trained on an SQL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dbcdb03-ce7b-45c2-b933-055bec0ffdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The ingredients for a meat pie can vary depending on the recipe, but some common ingredients include:\n",
      "\n",
      "* Flaky pie crust\n",
      "* Ground beef, pork, or a combination of the two\n",
      "* Onion\n",
      "* Garlic\n",
      "* Carrots\n",
      "* Peas\n",
      "* Thyme\n",
      "* Rosemary\n",
      "* Salt\n",
      "* Pepper\n",
      "* Egg wash (optional)\n",
      "\n",
      "Some recipes may also include other ingredients such as mushrooms, celery, or diced tomatoes.\n"
     ]
    }
   ],
   "source": [
    "generated_text = response_json[0]['generated_text']\n",
    "print(generated_text[len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032c5b2",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3df6c1",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27cf391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76e8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.43k/4.43k [00:00<00:00, 10.7MB/s]\n",
      "Downloading data: 100%|██████████| 21.8M/21.8M [00:02<00:00, 7.53MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 305.51it/s]\n",
      "Generating train split: 78577 examples [00:00, 241250.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('b-mc2/sql-create-context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3108c543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.47k/1.47k [00:00<00:00, 1.75MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 870kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 1.92MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 208kB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a18b63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'question', 'answer'],\n",
       "        num_rows: 78577\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24fb4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34892c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['context'],\n",
    "        examples['question'],\n",
    "        padding = 'max_length',\n",
    "        truncation = 'only_seond' #only allows the 'question' to be truncated if necessary due to constraints with size of input\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a9bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data'\n",
    "file_name = 'data_sql.jasonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f751c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/data_sql.jasonl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sql(data_dir: str = \"data_sql\"):\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    dataset = load_dataset(\"b-mc2/sql-create-context\")\n",
    "\n",
    "    dataset_splits = {\"train\": dataset[\"train\"]}\n",
    "    out_path = f'{directory}/{file_name}'\n",
    "\n",
    "    for key, ds in dataset_splits.items():\n",
    "        with open(out_path, \"w\") as f:\n",
    "            for item in ds:\n",
    "                newitem = {\n",
    "                    \"input\": item[\"question\"],\n",
    "                    \"context\": item[\"context\"],\n",
    "                    \"output\": item[\"answer\"],\n",
    "                }\n",
    "                f.write(json.dumps(newitem) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
